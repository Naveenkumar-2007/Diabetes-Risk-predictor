# ğŸš€ MLOps Integration - Complete Guide

## Overview
This diabetes prediction system now includes a **full MLOps pipeline** with automated training, monitoring, drift detection, and CI/CD.

## ğŸ¯ MLOps Features

### 1. **Experiment Tracking with MLflow**
- âœ… Track all model training experiments
- âœ… Compare model performance metrics
- âœ… Version control for models
- âœ… Model registry for production deployment

### 2. **Automated Model Training**
- âœ… Multi-model comparison (XGBoost, Random Forest, Gradient Boosting)
- âœ… Hyperparameter optimization
- âœ… Cross-validation
- âœ… Performance validation against thresholds

### 3. **Model Monitoring & Drift Detection**
- âœ… Real-time prediction logging
- âœ… Data drift detection using PSI (Population Stability Index)
- âœ… Concept drift detection (performance degradation)
- âœ… Automated retraining triggers

### 4. **CI/CD Pipeline with GitHub Actions**
- âœ… Automated model training on data updates
- âœ… Model testing and validation
- âœ… Performance threshold checks
- âœ… Automated deployment to production

### 5. **Data Versioning with DVC**
- âœ… Track data changes
- âœ… Reproducible experiments
- âœ… Data lineage

---

## ğŸ“ Project Structure

```
Diabetics-Agent/
â”œâ”€â”€ mlops/                          # MLOps components
â”‚   â”œâ”€â”€ model_trainer_mlops.py     # Training pipeline with MLflow
â”‚   â”œâ”€â”€ model_monitor.py            # Monitoring & drift detection
â”‚   â”œâ”€â”€ auto_retrain.py             # Automated retraining
â”‚   â””â”€â”€ dashboard_api.py            # MLOps dashboard API
â”œâ”€â”€ mlops_config.py                 # MLOps configuration
â”œâ”€â”€ .github/workflows/              # CI/CD pipelines
â”‚   â”œâ”€â”€ mlops-pipeline.yml         # Model training workflow
â”‚   â””â”€â”€ model-monitoring.yml       # Drift detection workflow
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_model.py              # Model tests for CI
â”œâ”€â”€ artifacts/                      # Model artifacts
â”‚   â”œâ”€â”€ model.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â””â”€â”€ model_metadata.json
â”œâ”€â”€ logs/predictions/               # Prediction logs for monitoring
â”œâ”€â”€ mlflow.db                       # MLflow tracking database
â””â”€â”€ .dvc/                          # DVC configuration
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
pip install dvc mlflow pytest
```

### 2. Initialize MLflow

```bash
# Start MLflow UI
mlflow ui --backend-store-uri sqlite:///mlflow.db

# Access at: http://localhost:5000
```

### 3. Train Model with MLOps

```bash
cd mlops
python model_trainer_mlops.py
```

This will:
- Track experiment in MLflow
- Train multiple models
- Select best model
- Save to model registry
- Deploy to production

### 4. Monitor Model Performance

```bash
python mlops/model_monitor.py
```

### 5. Trigger Automated Retraining

```bash
python mlops/auto_retrain.py
```

---

## ğŸ“Š MLOps Dashboard

### Start the Flask App with MLOps

```python
# In flask_app.py, add:
from mlops.dashboard_api import mlops_bp
app.register_blueprint(mlops_bp)
```

### Access MLOps APIs

- **Model Info**: `GET /mlops/api/model/info`
- **Performance Metrics**: `GET /mlops/api/monitoring/performance/<days>`
- **Drift Check**: `GET /mlops/api/monitoring/drift/check`
- **Recent Predictions**: `GET /mlops/api/monitoring/predictions/recent/<limit>`
- **Drift Alerts**: `GET /mlops/api/monitoring/alerts/recent`
- **Retrain History**: `GET /mlops/api/retrain/history`
- **Trigger Retrain**: `POST /mlops/api/retrain/trigger`

---

## ğŸ”„ Automated Workflows

### GitHub Actions Workflows

#### 1. **Model Training Pipeline** (`.github/workflows/mlops-pipeline.yml`)

Triggers:
- Weekly (Sunday midnight)
- Manual dispatch
- When data or MLOps code changes

Actions:
- Validate data
- Train models with MLflow
- Run tests
- Check performance thresholds
- Deploy if successful

#### 2. **Model Monitoring** (`.github/workflows/model-monitoring.yml`)

Triggers:
- Daily at 2 AM
- Manual dispatch

Actions:
- Check for data drift
- Check for concept drift
- Trigger retraining if needed

---

## ğŸ“ˆ Model Monitoring

### Data Drift Detection

Uses **PSI (Population Stability Index)**:

```python
from mlops.model_monitor import ModelMonitor

monitor = ModelMonitor()
drift_status = monitor.detect_data_drift()

if drift_status['drift_detected']:
    print("âš ï¸ Data drift detected!")
    print(f"Affected features: {drift_status['feature_psi']}")
```

**PSI Interpretation**:
- PSI < 0.1: No significant change
- PSI 0.1-0.25: Moderate change
- PSI > 0.25: Significant drift âš ï¸

### Concept Drift Detection

Monitors model accuracy over time:

```python
concept_drift = monitor.detect_concept_drift()

if concept_drift['drift_detected']:
    print(f"âš ï¸ Performance drop: {concept_drift['accuracy_drop']:.2%}")
```

### Automated Retraining Triggers

Retraining is triggered when:
1. **Data drift** detected (PSI > 0.05)
2. **Concept drift** detected (accuracy drop > 10%)
3. **Sufficient new data** (>100 labeled predictions)

---

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
# MLflow Configuration
MLFLOW_TRACKING_URI=sqlite:///mlflow.db

# Model Thresholds
MIN_MODEL_ACCURACY=0.75
MIN_MODEL_ROC_AUC=0.80
MIN_MODEL_F1=0.70

# Drift Detection
DRIFT_DETECTION_WINDOW=1000
DRIFT_THRESHOLD=0.05

# Auto Retraining
AUTO_RETRAIN_ENABLED=true
RETRAIN_SCHEDULE="0 0 * * 0"  # Weekly on Sunday
MIN_NEW_DATA_THRESHOLD=100
```

### MLOps Config (`mlops_config.py`)

Centralized configuration for:
- MLflow settings
- Model thresholds
- Drift parameters
- File paths

---

## ğŸ§ª Testing

### Run Model Tests

```bash
pytest tests/test_model.py -v
```

Tests include:
- âœ… Model artifact existence
- âœ… Model loading
- âœ… Performance thresholds
- âœ… Prediction functionality
- âœ… Input validation

### Test Coverage

```bash
pytest tests/ --cov=mlops --cov=src --cov-report=html
```

---

## ğŸ“¦ Data Versioning with DVC

### Initialize DVC

```bash
dvc init
```

### Track Data

```bash
# Track training data
dvc add data/raw/diabetes.csv

# Track model artifacts
dvc add artifacts/model.pkl
dvc add artifacts/scaler.pkl

# Commit changes
git add data/.gitignore artifacts/.gitignore data/raw/diabetes.csv.dvc
git commit -m "Track data and models with DVC"
```

### Push to Remote Storage

```bash
# Configure remote (Google Drive example)
dvc remote add -d storage gdrive://YOUR_FOLDER_ID

# Push data and models
dvc push
```

### Pull Latest Data/Models

```bash
dvc pull
```

---

## ğŸ” MLflow UI

### Start MLflow Server

```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001
```

### Access at `http://localhost:5001`

Features:
- ğŸ“Š Compare experiments
- ğŸ“ˆ View metrics (accuracy, ROC-AUC, F1)
- ğŸ·ï¸ Model registry
- ğŸ“ Parameters and artifacts

---

## ğŸš¨ Monitoring Alerts

### Drift Alerts

Logged in `logs/predictions/drift_alerts.jsonl`:

```json
{
  "timestamp": "2025-10-20T12:00:00",
  "alert_type": "data_drift",
  "drift_features": ["Glucose", "BMI"],
  "psi_scores": {"Glucose": 0.15, "BMI": 0.12},
  "action_required": "model_retraining_recommended"
}
```

### Performance Alerts

```json
{
  "timestamp": "2025-10-20T12:00:00",
  "alert_type": "concept_drift",
  "recent_accuracy": 0.72,
  "expected_accuracy": 0.85,
  "accuracy_drop": 0.13,
  "action_required": "immediate_model_retraining"
}
```

---

## ğŸ”„ Production Deployment

### 1. Update Model in Production

```bash
# Train new model
python mlops/model_trainer_mlops.py

# Model automatically saved to artifacts/
# Flask app automatically loads latest model on restart
```

### 2. Restart Flask App

```bash
python flask_app.py
```

### 3. Verify Deployment

```bash
curl http://localhost:5000/mlops/api/model/info
```

---

## ğŸ“Š Metrics & KPIs

### Model Performance Metrics

- **Accuracy**: Classification accuracy
- **ROC-AUC**: Area under ROC curve
- **F1-Score**: Harmonic mean of precision and recall
- **Precision**: True positives / (True positives + False positives)
- **Recall**: True positives / (True positives + False negatives)

### Operational Metrics

- **Prediction Volume**: Daily/weekly predictions
- **Prediction Latency**: Response time
- **Model Uptime**: Availability
- **Drift Score**: PSI values per feature

---

## ğŸ› Troubleshooting

### MLflow Not Starting

```bash
# Check if port is in use
lsof -i :5000  # or :5001

# Use different port
mlflow ui --port 5002
```

### Drift Detection Failing

```bash
# Create reference distribution
python -c "
from mlops.model_monitor import ModelMonitor
monitor = ModelMonitor()
monitor.create_reference_distribution('data/raw/diabetes.csv')
"
```

### Model Not Loading

```bash
# Check model file
ls -lh artifacts/model.pkl

# Re-train if corrupted
python mlops/model_trainer_mlops.py
```

---

## ğŸ“š Best Practices

1. **Always version your data** with DVC before training
2. **Track all experiments** in MLflow
3. **Set performance thresholds** in mlops_config.py
4. **Monitor drift daily** with GitHub Actions
5. **Test models** before deploying to production
6. **Log all predictions** for monitoring
7. **Retrain regularly** based on drift detection

---

## ğŸ¯ Next Steps

1. âœ… Set up MLflow tracking
2. âœ… Configure DVC for data versioning
3. âœ… Enable GitHub Actions workflows
4. âœ… Set up monitoring dashboard
5. âœ… Configure automated retraining
6. âœ… Deploy to production

---

## ğŸ“ Support

For issues or questions:
- Check logs in `logs/predictions/`
- Review MLflow experiments
- Check GitHub Actions runs
- Review drift alerts

---

## ğŸ† MLOps Maturity Level: **Level 3 - Automated**

- âœ… Version Control (Code, Data, Models)
- âœ… Automated Testing
- âœ… CI/CD Pipeline
- âœ… Automated Monitoring
- âœ… Drift Detection
- âœ… Automated Retraining
- âœ… Model Registry
- âœ… Experiment Tracking

**Status**: Production-Ready! ğŸš€
